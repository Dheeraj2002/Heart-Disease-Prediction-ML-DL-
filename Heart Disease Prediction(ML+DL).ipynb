{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Description**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"age: The person's age in years\n\nsex: The person's sex (1 = male, 0 = female)\n\ncp: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n\ntrestbps: The person's resting blood pressure (mm Hg on admission to the hospital)\n\nchol: The person's cholesterol measurement in mg/dl\n\nfbs: The person's fasting blood sugar (> 120 mg/dl, 1 = true; 0 = false)\n\nrestecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n\nthalach: The person's maximum heart rate achieved\n\nexang: Exercise induced angina (1 = yes; 0 = no)\n\noldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\n\nslope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n\nca: The number of major vessels (0-3)\n\nthal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n\ntarget: Heart disease (0 = no, 1 = yes)","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/heart-disease-uci/heart.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas_profiling as ppl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"profile = ppl.ProfileReport(df)\nprofile","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check for null values in the dataset. No needed as pandas_profiling has already done this job","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now , Check for the Correlation in the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.heatmap(df.corr(),cmap='viridis',annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check the Correlation of features with the target variable.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()['target'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following plot shows the Distribution of Age. This Graph tells that the highest number of people suffering from heart diseases are in the age group of 55-65 years.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('whitegrid')\nplt.figure(figsize=(10,5))\nsns.distplot(df['age'],color='cyan',kde=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now , Let's Look at target. It is such a quite balanced with almost equal number of both classes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['target'],palette='rainbow')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## It's time to do some other plots.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\nsns.boxplot(df['target'], df['trestbps'],hue=df['sex'], palette = 'viridis')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='target',hue='sex',data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='target',y='age',hue='sex',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The following function changes int-type categorical columns to object-type to perform OneHotEncoding (using pd.get_dummies). If we don't change them to object-type,after performing OneHotEncoding the values remains same.So that's why we changed them to object-type. Then we append the categorical column into categories .","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"categories = []\ndef categorical(df):\n    for column in df.drop('target',axis=1).columns :\n        if len(df[column].value_counts()) <10 and df[column].dtype != 'object': # and df[column].dtype != 'object' is no needed.\n            df[column] = df[column].astype('object')\n            categories.append(column)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = categorical(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating Dummy Variables for those categorical columns. Make sure that drop_first = True to avoid \"Dummy Variable Trap\".","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"onehot = pd.get_dummies(df[categories],drop_first = True)\nonehot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(categories,axis=1,inplace=True) # Removing those categorical columns\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('target',axis=1,inplace=True)\ndf = pd.concat([df,onehot],axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape,X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train[:,0:5] = sc.fit_transform(X_train[:,0:5])\nX_test[:,0:5] = sc.transform(X_test[:,0:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier,VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,accuracy_score,classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier()\nrf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = rf.predict(X_test)\nconfusion_matrix(y_test,predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter Tuning Starts...!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Tuning Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators = [200,300,400,500,600,700]\nmax_depth = range(1,12)\ncriterions = ['gini', 'entropy']\nparameters = {'n_estimators':n_estimators,\n              'max_depth':max_depth,\n              'criterion': criterions\n              }\ngrid = GridSearchCV(estimator=RandomForestClassifier(max_features='auto',n_jobs=-1),\n                    param_grid=parameters,\n                    cv=5,\n                    verbose=1,\n                    n_jobs = -1)\ngrid.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_grid = grid.best_estimator_\nrf_grid.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = rf_grid.predict(X_test)\nconfusion_matrix(y_test,predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's look at some important features...!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances = pd.DataFrame(rf_grid.feature_importances_,\n                                   index=df.columns,\n                                   columns=['importance'])\nfeature_importances.sort_values(by='importance', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tuning Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"C_vals = [0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,3,3.2,3.6,\n          4,5,6,7,8,9,10]\npenalties = ['l1','l2']\nsolvers = ['liblinear', 'sag','lbfgs']\nparameters = {'penalty': penalties, 'C': C_vals, 'solver':solvers}\n\ngrid = GridSearchCV(estimator=LogisticRegression(),\n                    param_grid=parameters,\n                    scoring='accuracy',\n                    cv=5,\n                    verbose=1,\n                    n_jobs=-1)\ngrid.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_grid = grid.best_estimator_\nlr_grid.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = lr_grid.predict(X_test)\nconfusion_matrix(y_test,predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tuning SVM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"C = [0.01, 0.1, 1,1.2,1.5,2,2.5,3,3.2,3.5,4]\ngamma = [0.0001,0.001,0.005, 0.01, 0.1, 1]\nparameters = {'C': C, 'gamma' : gamma}\ngrid = GridSearchCV(estimator=SVC(kernel = 'rbf', probability=True),\n                    param_grid=parameters,\n                    scoring='accuracy',\n                    verbose=1,\n                    cv=5,\n                    n_jobs=-1)\ngrid.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_grid = grid.best_estimator_\nsvm_grid.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = svm_grid.predict(X_test)\nconfusion_matrix(y_test,predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances = pd.DataFrame(rf_grid.feature_importances_,\n                                   index=df.columns,\n                                    columns=['importance'])\nfeature_importances.sort_values(by='importance', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tuning Bagging Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators = [200,300,330,370,400,430,470,500,600,700]\n\n\nparameters = {'n_estimators':n_estimators}\n\ngrid = GridSearchCV(BaggingClassifier(base_estimator= None),\n                                 param_grid=parameters,\n                                 cv=5,verbose=1,\n                                 n_jobs = -1)\ngrid.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bag_grid = grid.best_estimator_\nbag_grid.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = bag_grid.predict(X_test)\nconfusion_matrix(y_test,predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tuning XGBClassifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"base_score = [0.1,0.3,0.5,0.7,0.9]\nmax_depth = range(4,15)\nlearning_rate = [0.01,0.1,0.2,0.3,0.4]\ngamma = [0.001,0.01,0.1,0.3,0.5]\nparameters = {'base_score':base_score,\n              'max_depth':max_depth,\n              'learning_rate': learning_rate,\n              'gamma':gamma\n              }\ngrid = GridSearchCV(estimator=XGBClassifier(n_jobs=-1),\n                    param_grid=parameters,\n                    cv=5,\n                    verbose=1,\n                    n_jobs = -1)\ngrid.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_grid = grid.best_estimator_\nxgb_grid.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = xgb_grid.predict(X_test)\nconfusion_matrix(y_test,predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now, Combine all of them using Voting Classifier...!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vot_clf = VotingClassifier(estimators=[('rf',rf_grid),\n                                       ('lr',lr_grid),\n                                       ('svc',svm_grid),\n                                       ('bag',bag_grid),\n                                       ('xgb',xgb_grid)], voting='hard')\nvot_clf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = vot_clf.predict(X_test)\nconfusion_matrix(y_test,predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vot_clf.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_grid.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bag_grid.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_grid.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Let's use Artificial Neural Network (ANN) ...!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Dense(units=30,activation = 'relu' ,input_shape=(22,)))\n\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(units=15,activation = 'relu'))\n\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(units=7,activation = 'relu'))\n\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(units=1,activation = 'sigmoid'))\n\n\nmodel.compile(optimizer = 'adam',loss='binary_crossentropy',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x=X_train, \n                    y=y_train, \n                    epochs=200,\n                    validation_data=(X_test, y_test),\n                    verbose=1,\n                    callbacks=[early_stop]\n                    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = [1 if i>0.5 else 0 for i in predictions]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test,predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tuning ANN Using GridSearch ....!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom tensorflow.keras.layers import BatchNormalization","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create a function to build our ANN model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Keras provides a wrapper class KerasClassifier that allows us to use our deep learning models with scikit-learn, this is especially useful when you want to tune hyperparameters using scikit-learn's RandomizedSearchCV or GridSearchCV.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(layers,dropout_rate=0):\n    model = Sequential()\n    for i,nodes in enumerate(layers):\n        if i==0:\n            model.add(Dense(nodes,activation='relu',input_dim=X_train.shape[1]))\n        else :\n            model.add(Dense(nodes,activation='relu'))\n            \n        model.add(BatchNormalization())\n        \n        if dropout_rate:\n            model.add(Dropout(dropout_rate))\n    \n    model.add(Dense(1,activation='sigmoid'))\n    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n    return model\n\n    \nmodel = KerasClassifier(build_fn=build_model,verbose=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define the parameters when we fit our ANN except X and y , such as epochs,callbacks etc.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\ncallbacks = [early_stop]\n\nfit_parameters = {'callbacks': callbacks,\n                  'epochs': 200,\n                  'validation_data' : (X_test,y_test),\n                  'verbose' : 0}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define some of the Hyperparameters of our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"layers = [(15,1),(20,10,1),(30,15,7,1)]\n\nparameters = dict(layers=layers,dropout_rate=[0,0.1,0.2,0.3],batch_size=[32,64,128,256])\n\ngrid = GridSearchCV(estimator=model,\n                    param_grid=parameters,\n                    cv=5,\n                    verbose=1,\n                    n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### To fit the fit_params we have to do \"**fit_params\"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.fit(X_train,y_train,**fit_parameters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = grid.predict(X_test)\nconfusion_matrix(y_test,predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### I had used grid for every tuned model.But Below grid has the tuned ANN model because it is the latest one.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all_models = [rf_grid,\n              lr_grid,\n              svm_grid,\n              bag_grid,\n              xgb_grid,\n              vot_clf,\n              grid]\nc = {}\nfor i in all_models :\n    a = i.predict(X_test)\n    b = accuracy_score(y_test,a)\n    c[i] = b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final Prediction !!!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = (max(c,key=c.get)).predict(X_test)\n\nconfusion_matrix(y_test,predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Save and Load the Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### I saved the vot_clf model because ANN or any Deep Learning model can be saved in the h5 file format.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = 'model.pkl'\npickle.dump(vot_clf, open(filename, 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loaded_model = pickle.load(open(filename, 'rb'))\npredictions = loaded_model.predict(X_test)\nconfusion_matrix(y_test,predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Thank you !!! for your Patience. If this notebook is helpful , please Upvote .....!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}